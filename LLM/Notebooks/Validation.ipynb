{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5v4BnKcoSl7",
        "outputId": "dfcc4465-c03c-4dbf-881e-438aebac7b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
            "   \\\\   /|    GPU: NVIDIA RTX A4000. Max memory: 15.635 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu118. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1+cu118. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True #  4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    #model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    model_name=\"a-hamdi/NGILlama3-merged\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhK-FesPoSl9"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an article that describes a news. Write a response that appropriately completes the request.\n",
        "\n",
        "### Article:\n",
        "{}\n",
        "\n",
        "\n",
        "### Label:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    articles = examples[\"Article\"]\n",
        "    labels      = examples[\"label\"]\n",
        "    texts = []\n",
        "    for Article,Label in zip(articles,labels):\n",
        "        # Must add EOS_TOKEN, otherwise the generation will go on forever!\n",
        "        text = alpaca_prompt.format(Article, Label) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "url='TheFullDataset.csv'\n",
        "#i'm only selecting 1000! we should remove that line when real finetuning\n",
        "dataset = load_dataset(\"csv\", data_files = {\"train\" : url}, split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "length_dataset=len(dataset)\n",
        "val_dataset=dataset.select(indices=range(length_dataset-50000,length_dataset))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-bftX4noSl-",
        "outputId": "b3f40ea3-8a93-49bf-96cf-5417bd295bbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an article that describes a news. Write a response that appropriately completes the request.\n",
            "\n",
            "### Article:\n",
            "LASIK warning controversy sparks debate on telivision on how dangerous it is, and they concluded it is lethal.\n",
            "\n",
            "\n",
            "### Label:\n",
            "Fake<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"LASIK warning controversy sparks debate on telivision on how dangerous it is, and they concluded it is lethal.\", # article\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unfSElVIoSl-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import contextlib\n",
        "def get_prediction(article):\n",
        "    # Suppress stdout\n",
        "    f = io.StringIO()\n",
        "    with contextlib.redirect_stdout(f):\n",
        "        prompt = alpaca_prompt.format(article, \"\", \"\")\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    # Resume normal stdout\n",
        "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    # Assuming the model's output is either \"Fake\" or \"Reliable\"\n",
        "    return generated_text.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEL4mSl-oSl-",
        "outputId": "dfd50cc3-4489-4c8b-a080-a660a246f71c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'reliable'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output=get_prediction(\"warning controversy sparks debate on telivision on how dangerous it is, and they concluded it is lethal.\")\n",
        "output[-8:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmZ-JkjOoSl_",
        "outputId": "e28b151e-5bb7-4299-a33e-138a7523d590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "validating the model:   0%|          | 0/50000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "validating the model: 100%|██████████| 50000/50000 [4:50:11<00:00,  2.87it/s]  \n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "# Lists to store true labels and predictions\n",
        "true_labels = []\n",
        "predictions = []\n",
        "def preprocess_text(article):\n",
        "    # Remove the \"### Label\" section\n",
        "    article = re.sub(r'### Label:.*', '', article)\n",
        "\n",
        "    # Remove words like \"reliable\" or \"fake\" (case insensitive)\n",
        "    article = re.sub(r'\\b(?:reliable|fake)\\b', '', article, flags=re.IGNORECASE)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    return article.strip()\n",
        "true_labels = []\n",
        "predictions = []\n",
        "# Iterate over the evaluation dataset\n",
        "for example in tqdm(val_dataset,desc=\"validating the model\"):\n",
        "    article = preprocess_text(example[\"text\"])\n",
        "    true_labell = example[\"label\"].lower()  # Assuming the label field in your dataset is \"label\"\n",
        "    if \"fake\" in true_labell:\n",
        "      true_label=0\n",
        "    else:\n",
        "      true_label=1\n",
        "\n",
        "    # Get the model's prediction\n",
        "    predictionn = get_prediction(article)\n",
        "\n",
        "\n",
        "    if \"fake\" in predictionn:\n",
        "      prediction=0\n",
        "    else:\n",
        "      prediction=1\n",
        "    # Append to lists\n",
        "    true_labels.append(true_label)\n",
        "    predictions.append(prediction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiIRxqOyoSl_",
        "outputId": "bc6bb24c-bfb1-4372-837e-ef4f8a153ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[26136     2]\n",
            " [   32 23830]]\n",
            "Accuracy: 0.9993\n",
            "Precision: 0.9999\n",
            "Recall: 0.9987\n",
            "F1 Score: 0.9993\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=[0, 1])\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(true_labels, predictions)\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(true_labels, predictions, labels=[0, 1])\n",
        "recall = recall_score(true_labels, predictions, labels=[0, 1])\n",
        "f1 = f1_score(true_labels, predictions, labels=[0, 1])\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xA70TJxoSl_",
        "outputId": "2254797f-d243-45f9-e10d-d184a5b622d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[26136     2]\n",
            " [   32 23830]]\n",
            "Accuracy: 0.9993\n",
            "Precision: 0.9999\n",
            "Recall: 0.9987\n",
            "F1 Score: 0.9993\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=[0, 1])\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(true_labels, predictions)\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(true_labels, predictions, labels=[0, 1])\n",
        "recall = recall_score(true_labels, predictions, labels=[0, 1])\n",
        "f1 = f1_score(true_labels, predictions, labels=[0, 1])\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuSxyoZmoSmA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'True Label': true_labels, 'Prediction': predictions})\n",
        "df.to_csv('predictions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "unsloth2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}